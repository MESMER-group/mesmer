{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ prolog }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating MESMER on multiple scenarios\n",
    "This tutorial shows how to calibrate the parameters for MESMER on an example dataset of coarse regridded ESM output for multiple climate change scenarios. We calibrate the parameters for MESMER using three scenarios: a historical, a low emission (SSP1-2.6), and a high emission (SSP5-8.5) scenario, where SSP5-8.5 includes several ensemble members. You can find the basics of the MESMER approach in Beusch et al. ([2020](https://doi.org/10.5194/ESD-11-139-2020)) and the multi-sceario approach in Beusch et al. ([2022](https://doi.org/10.5194/gmd-15-2085-2022)). Training MESMER consists of four steps:\n",
    "\n",
    "- **global trend**: compute the global temperature trend, including the volcanic influence on historical trends\n",
    "- **global variablity**: estimating the parameters to generate global variability\n",
    "- **local trend**: estimate parameters to translate global mean temperature (including global variability) into local temperature\n",
    "- **local variability**: estimate parameters needed to generate local variability\n",
    "\n",
    "This example can be extended to more scenarios, ensemble members and higher resolution data. See also the mesmer calibration test in *tests/integration/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import filefisher\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import mesmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MESMER expects a specific data format. Data from each scenario should be a node (or group) on an `xr.DataTree` (more on this below) e.g.:\n",
    "\n",
    "```\n",
    "<xarray.DataTree>\n",
    "Group: /\n",
    "├── Group: /historical\n",
    "|    ...\n",
    "├── Group: /ssp126\n",
    "|    ...\n",
    "```\n",
    "\n",
    "Each scenario is a `xr.Dataset` with 4 dimensions: `member`, `time`, `lat`, `lon`. Below we show one way to load data such that it conforms to the desired format. We load data from the cmip6-ng (\"new generation\") repository. This data has undergone a small reformatting from the original cmip6 archive. For the sake of computational speed we also load data which has been regridded to a coarse resolution. Loading the data can be adapted to the data format you are most used to - as long as the final output has the desired format.\n",
    "\n",
    "---\n",
    "\n",
    "MESMER is Earth System Model specific, aiming to reproduce the behaviour of one ESM. Here we train on the CMIP6 output of the model IPSL-CM6A-LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"IPSL-CM6A-LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the library [*filefisher*](https://github.com/mpytools/filefisher) to search all files in the cmip6-ng archive for the model and scenarios we want to use. Filefisher can search through paths for given file patterns. It returns all paths matching the pattern such that you can load the files in the next step.\n",
    "\n",
    "Here, we want to find all files that have data for annual near surface temperature (`\"tas\"`) for the used model and the future scenarios ssp126 and ssp585. Next, we search for the historical data that match the members found for the two future scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesmer provides example data under \"./data/cmip6-ng\"\n",
    "cmip_data_path = mesmer.example_data.cmip6_ng_path(relative=True)\n",
    "\n",
    "CMIP_FILEFINDER = filefisher.FileFinder(\n",
    "    path_pattern=cmip_data_path / \"{variable}/{time_res}/{resolution}\",\n",
    "    file_pattern=\"{variable}_{time_res}_{model}_{scenario}_{member}_{resolution}.nc\",\n",
    ")\n",
    "CMIP_FILEFINDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search data for ssp126 and ssp585 - we find one and two ensemble members, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"ssp126\", \"ssp585\"]\n",
    "\n",
    "keys = {\"variable\": \"tas\", \"model\": model, \"resolution\": \"g025\", \"time_res\": \"ann\"}\n",
    "\n",
    "fc_scens = CMIP_FILEFINDER.find_files(scenario=scenarios, keys=keys)\n",
    "fc_scens.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to find the same ensemble members in the historical data, such that we end up with five files we need to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the historical members that are also in the future scenarios, but only once\n",
    "members = fc_scens.df.member.unique()\n",
    "\n",
    "fc_hist = CMIP_FILEFINDER.find_files(scenario=\"historical\", member=members, keys=keys)\n",
    "\n",
    "fc_all = fc_hist.concat(fc_scens)\n",
    "fc_all.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load all the files we found into a ``DataTree``, a data structure provided by [xarray](https://docs.xarray.dev/en/stable/index.html). It is a container to hold xarray `Dataset` objects that are not alignable. This is useful for us since we have historical and future data, which have different time coordinates. Moreover, the scenarios may also have different numbers of members (as e.g., SSP1-2.6, which only has one). Thus, we store the data of each scenario in a `Dataset` with all its ensemble members along a `member` dimension. Then we store all the scenario datasets in one `DataTree` node. The `DataTree` allows us to perform computations on each of the scenarios separately.\n",
    "\n",
    "We define a helper function to load the data from the cmip6_ng example data repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filecontainer):\n",
    "\n",
    "    out = xr.DataTree()\n",
    "\n",
    "    scenarios = filecontainer.df.scenario.unique().tolist()\n",
    "\n",
    "    # load data for each scenario\n",
    "    for scen in scenarios:\n",
    "        files = filecontainer.search(scenario=scen)\n",
    "\n",
    "        # load all members for a scenario\n",
    "        members = []\n",
    "        for fN, meta in files.items():\n",
    "            time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n",
    "            ds = xr.open_dataset(fN, decode_times=time_coder)\n",
    "            # drop unnecessary variables\n",
    "            ds = ds.drop_vars([\"height\", \"time_bnds\", \"file_qf\"], errors=\"ignore\")\n",
    "            # assign member-ID as coordinate\n",
    "            ds = ds.assign_coords({\"member\": meta[\"member\"]})\n",
    "            members.append(ds)\n",
    "\n",
    "        # create a Dataset that holds each member along the member dimension\n",
    "        scen_data = xr.concat(members, dim=\"member\")\n",
    "        # put the scenario dataset into the DataTree\n",
    "        out[scen] = xr.DataTree(scen_data)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = load_data(fc_all)\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in the data format discussed above. You can examine it by clicking on `Groups` above. \n",
    "\n",
    "---\n",
    "\n",
    "We will need some configuration parameters in the following:\n",
    "1. ``THRESHOLD_LAND``: threshold above which land fraction to consider a grid point as a land grid point.\n",
    "2. ``REFERENCE_PERIOD``: we will work not with absolute temperature values but with temperature anomalies w.r.t. a reference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_LAND = 1 / 3\n",
    "REFERENCE_PERIOD = slice(\"1850\", \"1900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies w.r.t. the reference period\n",
    "tas_anom = mesmer.anomaly.calc_anomaly(dt, reference_period=REFERENCE_PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global mean\n",
    "tas_globmean = mesmer.weighted.global_mean(tas_anom)\n",
    "tas_globmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the **global trend** and **global variability** we split the global mean temperature signal into a trend and variability component:\n",
    "$T_{t}^{glob} = T_{t}^{glob,\\,trend} + T_{t}^{glob,\\,var}$.\n",
    "The trend component is further split into a smooth and volcanic component:\n",
    "$T_{t}^{glob,\\,trend} = T_{t}^{glob,\\,smooth} + T_{t}^{glob,\\,volc}$.\n",
    "\n",
    "\n",
    "## \"Smooth\" and volcanic components of the global temperature\n",
    "The volcanic contributions to the global mean temperature trend of the historical period have to be removed to estimate the linear regression of global mean temperature to local temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate $T_{t}^{glob,\\,smooth}$ using a lowess smoother, with 50 time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean over members before smoothing\n",
    "tas_globmean_ensmean = tas_globmean.mean(dim=\"member\")\n",
    "\n",
    "n_steps = 50\n",
    "\n",
    "tas_globmean_smoothed = mesmer.stats.lowess(\n",
    "    tas_globmean_ensmean,\n",
    "    dim=\"time\",\n",
    "    n_steps=n_steps,\n",
    "    use_coords=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "h0, *_ = tas_globmean[\"historical\"].tas.plot.line(ax=ax, x=\"time\", color=\"grey\", lw=1)\n",
    "a2, *_ = tas_globmean_smoothed[\"historical\"].tas.plot.line(ax=ax, x=\"time\", lw=2)\n",
    "\n",
    "ax.legend([h0, a2], [\"Ensemble members\", \"Smooth ensemble mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit the parameter of the volcanic contributions only on the historical smoothed data of all ensemble members available. The future scenarios do not have volcanic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_tas_residuals = tas_globmean[\"historical\"] - tas_globmean_smoothed[\"historical\"]\n",
    "\n",
    "# fit volcanic influence\n",
    "volcanic_params = mesmer.volc.fit_volcanic_influence(hist_tas_residuals.tas)\n",
    "\n",
    "volcanic_params.aod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Superimpose the volcanic influence on the historical time series. Because the historical data is treated as its own scenario, we encounter discontinuities at the boundary between historical and future period. However, this is not relevant for the fitting of the parameters hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superimpose the volcanic forcing on historical data\n",
    "tas_globmean_smoothed[\"historical\"] = mesmer.volc.superimpose_volcanic_influence(\n",
    "    tas_globmean_smoothed[\"historical\"],\n",
    "    volcanic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot global mean time series\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "# plot unsmoothed global means\n",
    "tas_globmean[\"historical\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"0.5\", add_legend=False\n",
    ")\n",
    "tas_globmean[\"ssp126\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"#6baed6\", add_legend=False\n",
    ")\n",
    "tas_globmean[\"ssp585\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"#fc9272\", add_legend=False\n",
    ")\n",
    "\n",
    "# plot smoothed global means including volcanic influence for historical\n",
    "tas_globmean_smoothed[\"historical\"].tas.plot.line(\n",
    "    ax=ax, lw=1.5, x=\"time\", color=\"0.1\", label=\"historical\"\n",
    ")\n",
    "tas_globmean_smoothed[\"ssp126\"].tas.plot.line(\n",
    "    ax=ax, lw=1.5, x=\"time\", color=\"#08519c\", label=\"ssp126\"\n",
    ")\n",
    "tas_globmean_smoothed[\"ssp585\"].tas.plot.line(\n",
    "    ax=ax, lw=1.5, x=\"time\", color=\"#de2d26\", label=\"ssp585\"\n",
    ")\n",
    "\n",
    "# histend = tas_globmean[\"historical\"].time.isel(time=-1).item()\n",
    "# ax.axvline(histend, color=\"0.4\")\n",
    "ax.axhline(0, color=\"0.1\", lw=0.5)\n",
    "\n",
    "ax.set_title(\"\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate residuals (w.r.t. smoothed ts) i.e. remove the smoothed global mean, including the volcanic influence from the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_globmean_resids = tas_globmean - tas_globmean_smoothed\n",
    "# rename to tas_resids\n",
    "tas_globmean_resids = mesmer.datatree.map_over_datasets(\n",
    "    lambda ds: ds.rename({\"tas\": \"tas_resids\"}), tas_globmean_resids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "h0, *_ = tas_globmean_resids[\"historical\"].tas_resids.plot.line(\n",
    "    x=\"time\", color=\"0.5\", lw=1, add_legend=False\n",
    ")\n",
    "h1, *_ = tas_globmean_resids[\"ssp126\"].tas_resids.plot.line(\n",
    "    x=\"time\", color=\"#08519c\", lw=1, add_legend=False\n",
    ")\n",
    "h2, *_ = tas_globmean_resids[\"ssp585\"].tas_resids.plot.line(\n",
    "    x=\"time\", color=\"#de2d26\", lw=1, add_legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Residuals\")\n",
    "plt.axhline(0, lw=1, color=\"0.1\")\n",
    "\n",
    "plt.legend([h0, h1, h2], [\"historical\", \"ssp126\", \"ssp585\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to fit an AR process for estimating global variability, taking in the residual global mean temperature as follows:\n",
    "\n",
    "$$T_{t}^{glob,\\,var} = \\alpha_0 + \\sum\\limits_{k=1}^{p} \\alpha_k \\cdot T_{t-k}^{glob,\\,var} + \\varepsilon_t,\\ \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the order of the AR process and then fit the parameters. Internally, we fit the parameters for each member and then average first over the parameters of each scenario and then over all scenarios to arrive at a single set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_order = mesmer.stats.select_ar_order_scen_ens(\n",
    "    tas_globmean_resids, dim=\"time\", ens_dim=\"member\", maxlag=12, ic=\"bic\"\n",
    ")\n",
    "\n",
    "global_ar_params = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    tas_globmean_resids, dim=\"time\", ens_dim=\"member\", lags=ar_order\n",
    ")\n",
    "\n",
    "global_ar_params = global_ar_params.drop_vars(\"nobs\")\n",
    "global_ar_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local forced response\n",
    "Now we need to estimate how the global trend translates into a local forced response. This is done using a linear regression of the global trend and the global variability as predictors:\n",
    "\n",
    "$T_{s,t}^{resp} = \\beta_s^{int} + \\beta_s^{trend} \\cdot T_t^{glob,\\,trend} + \\beta_s^{var} \\cdot T_t^{glob,\\,var}$\n",
    "\n",
    "To this end, we stack all values (members, scenarios) into a single dataset, the only important thing is that predictor and predicted values stay together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the coefficients we need to prepare the local temperature data:\n",
    "\n",
    "1. Mask out ocean grid points (where the land fraction is larger than `THRESHOLD_LAND`)\n",
    "2. Mask out Antarctica\n",
    "3. Convert the data from a 2D lat-lon grid to a 1D grid by stacking it and removing all gridcells that were previously masked out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before stacking, we extract the original grid. We need to save this together with the parameters to later be able to reconstruct the original grid from the gridpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract original grid\n",
    "grid_orig = tas_anom[\"historical\"].ds[[\"lat\", \"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_stack(dt, threshold_land):\n",
    "    dt = mesmer.mask.mask_ocean_fraction(dt, threshold_land)\n",
    "    dt = mesmer.mask.mask_antarctica(dt)\n",
    "    dt = mesmer.grid.stack_lat_lon(dt)\n",
    "    return dt\n",
    "\n",
    "\n",
    "# mask and stack the data\n",
    "tas_stacked = mask_and_stack(tas_anom, THRESHOLD_LAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_stacked[\"ssp585\"].tas.isel(member=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now converted the 3D field (with dimensions lat, lon, and time) to a 2D field (with dimensions gridcell and time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `DataTree` from all predictors - here the smoothed global mean and it's residuals. We could add more predictors, e.g. the squared temperatures or the ocean heat uptake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = mesmer.datatree.merge([tas_globmean_smoothed, tas_globmean_resids])\n",
    "\n",
    "target = tas_stacked.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression, we want to weight the values of the different scenarios equally, i.e. we do not want scenarios with more members (here ssp585) be overrepresented in the linear regression parameters. Thus, we generate weights that weigh each value by the number of members in their scenario, so $w_{scen, mem, ts} = 1 / n\\_mem_{scen}$. We do currently not take different number of timesteps (historical vs. scenario) into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weights\n",
    "weights = mesmer.weighted.equal_scenario_weights_from_datatree(tas_stacked)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pool the different scenarios, ensemble members and timesteps into one sample dimension (containing time, member, and scenario as coordinates) for the linear regression. We want one `DataArray` per predictor and the target such that each sample of the predictor variables aligns with the corresponding sample of the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_pooled, target_pooled, weights_pooled = (\n",
    "    mesmer.datatree.broadcast_and_pool_scen_ens(predictors, target, weights)\n",
    ")\n",
    "\n",
    "target_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression, the predictors for each sample are used for every gridpoint of the target. We can now fit the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_lin_reg = mesmer.stats.LinearRegression()\n",
    "\n",
    "local_lin_reg.fit(\n",
    "    predictors=predictors_pooled,\n",
    "    target=target_pooled.tas,\n",
    "    dim=\"sample\",\n",
    "    weights=weights_pooled.weights,\n",
    ")\n",
    "\n",
    "local_forced_response_params = local_lin_reg.params\n",
    "local_forced_response_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars = (\n",
    "    \"intercept\",\n",
    "    \"tas\",\n",
    "    \"tas_resids\",\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(\n",
    "    3, 1, sharex=True, sharey=True, subplot_kw={\"projection\": ccrs.Robinson()}\n",
    ")\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, data_var in zip(axs, data_vars):\n",
    "\n",
    "    da = local_forced_response_params[data_var]\n",
    "    da = mesmer.grid.unstack_lat_lon_and_align(da, grid_orig)\n",
    "\n",
    "    h = da.plot(\n",
    "        ax=ax,\n",
    "        label=data_var,\n",
    "        robust=True,\n",
    "        center=0,\n",
    "        extend=\"both\",\n",
    "        add_colorbar=False,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "    ax.set_extent((-180, 180, -60, 85), ccrs.PlateCarree())\n",
    "    cbar = plt.colorbar(h, ax=ax, extend=\"both\", pad=0.025)  # , shrink=0.7)\n",
    "    ax.set(title=data_var, xlabel=\"\", ylabel=\"\", xticks=[], yticks=[])\n",
    "    ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local variability\n",
    "Next we to fit the parameters for the local AR(1) process with a spatially correlated noise term used to emulate local variability:\n",
    "\n",
    "$\\eta_{s,\\,t} = \\gamma_{0,\\,s} + \\gamma_{1,\\,s} \\cdot \\eta_{s,\\,t-1} + \\nu_{s,\\,t}, \\ \\nu_{s,\\,t} \\sim \\mathcal{N}(0, \\Sigma_{\\nu}(r))$\n",
    "\n",
    "The first component which contains the AR parameters ($\\gamma_{0,\\,s} + \\gamma_{1,\\,2} \\cdot \\eta_{s,\\,t-1}$) ensures temporal correlation of the local variability whereas the noise term $\\nu_{s,\\,t}$ ensures spatial consistency. The covariance matrix $\\Sigma_{\\nu}(r)$ is estimated on the whole grid and represents the spatial correlation of temperatures between the different gridpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the AR parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to compute the residuals after the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = local_lin_reg.residuals(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local AR(1) process is estimated on the individual scenarios, but the covariance is estimated on the pooled residuals - therefore we need to have them in both forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids_pooled = mesmer.datatree.pool_scen_ens(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the AR(1) process\n",
    "local_ar = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    resids,\n",
    "    ens_dim=\"member\",\n",
    "    dim=\"time\",\n",
    "    lags=1,\n",
    ")\n",
    "\n",
    "local_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate covariance matrix\n",
    "For the covariance matrix of the white noise we first estimate the empirical covariance matrix of the gridcell's values and then localize it using the Gaspari-Cohn function. This function goes to 0 for for larger distances  and becomes exactly 0 for distances twice the so called localisation radius. This is also called regularization. It ensures that grid points that are further away from each other do not correlate. Such spurious correlations can arise from rank deficient covariance matrices. In our case because we estimate the covariance on data that has more gridcells than timesteps.\n",
    "\n",
    "The localisation radius is a parameter that needs to be calibrated and we find the best localisation radius by cross-validation of several radii using the negative loglikelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the distance matrix - the distance between the gridpoints in km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_stacked = resids[\"historical\"].ds[[\"lat\", \"lon\"]]\n",
    "geodist = mesmer.geospatial.geodist_exact(grid_stacked.lon, grid_stacked.lat)\n",
    "\n",
    "# plot\n",
    "f, ax = plt.subplots()\n",
    "geodist.plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. prepare the localizer(s) to regularize the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_gc_localizer = mesmer.stats.gaspari_cohn_correlation_matrices(\n",
    "    geodist, range(5_000, 15_001, 500)\n",
    ")\n",
    "\n",
    "# plot one\n",
    "f, ax = plt.subplots()\n",
    "phi_gc_localizer[5000].plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing weights from local trend regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. find the best localization radius and localize the empirical covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = \"sample\"\n",
    "k_folds = 15\n",
    "\n",
    "localized_ecov = mesmer.stats.find_localized_empirical_covariance(\n",
    "    resids_pooled.residuals,\n",
    "    weights_pooled.weights,\n",
    "    phi_gc_localizer,\n",
    "    dim,\n",
    "    k_folds=k_folds,\n",
    ")\n",
    "\n",
    "localized_ecov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, sharey=True, constrained_layout=True)\n",
    "\n",
    "opt = dict(vmin=0, vmax=1.5, cmap=\"Blues\", add_colorbar=False)\n",
    "\n",
    "ax = axs[0]\n",
    "localized_ecov.covariance.plot(ax=ax, **opt)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Empirical covariance\")\n",
    "\n",
    "ax = axs[1]\n",
    "localized_ecov.localized_covariance.plot(ax=ax, **opt)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Localized empirical covariance\")\n",
    "ax.set_ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Adjust the regularized covariance matrix\n",
    "\n",
    "   Lastly we need to adjust the localized covariance matrix using the AR(1) parameters since the variance of the time series we observe is bigger than the variance of the driving white noise process. Read more about this in: \"Statistical Analysis in Climate Research\" by Storch and Zwiers (1999, reprinted 2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localized_covariance_adjusted = mesmer.stats.adjust_covariance_ar1(\n",
    "    localized_ecov.localized_covariance, local_ar.coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have calibrated all needed parameters and can save them. We can use filefisher to nicely create file names and save the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path relative to this notebook & create folder\n",
    "param_path = pathlib.Path(\"./output/calibrated_parameters/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_FILEFINDER = filefisher.FileFinder(\n",
    "    path_pattern=param_path / \"{esm}_{scen}\",\n",
    "    file_pattern=\"params_{module}_{esm}_{scen}.nc\",\n",
    ")\n",
    "\n",
    "scen_str = \"-\".join(scenarios)\n",
    "\n",
    "folder = PARAM_FILEFINDER.create_path_name(esm=model, scen=scen_str)\n",
    "pathlib.Path(folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "params = {\n",
    "    \"volcanic\": volcanic_params,\n",
    "    \"global-variability\": global_ar_params,\n",
    "    \"local-trends\": local_lin_reg,\n",
    "    \"local-variability\": local_ar,\n",
    "    \"covariance\": localized_ecov,\n",
    "    \"grid-orig\": grid_orig,\n",
    "}\n",
    "\n",
    "\n",
    "save_files = False  # we don't save them here in the example\n",
    "if save_files:\n",
    "\n",
    "    for module, param in params.items():\n",
    "\n",
    "        filename = PARAM_FILEFINDER.create_full_name(\n",
    "            module=module,\n",
    "            esm=model,\n",
    "            scen=scen_str,\n",
    "        )\n",
    "\n",
    "        param.to_netcdf(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use the calibrated parameters for emulation, see the Tutorials for emulating one or multiple scenarios in the Tutorial section next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesmer_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
