{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbf1855",
   "metadata": {},
   "source": [
    "{{ prolog }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618c9e1",
   "metadata": {},
   "source": [
    "# MESMER-X workflow for annual maximum temperatures\n",
    "\n",
    "In this tutorial we use a conditional distribution to model the dependence of a climate variable on a global predictor using the approach from MESMER-X (Quilquaille et al., ). In this example we model annual maximum temperature with a normal distribution with a linear dependence on global mean temperature. \n",
    "\n",
    ":::{note}\n",
    "- We fit annual maximum data (i.e. a block maxima) using a normal distribution. This is not the appropriate distribution for this type of data but is used for illustrative and computational time purposes.\n",
    "- A normal distribution with a linear dependence of the mean on a variable is equivalent to a linear regression with is much faster to use directly. Again, this is only for illustrative purposes.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fec2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import mesmer\n",
    "from mesmer.distrib import (\n",
    "    ConditionalDistribution,\n",
    "    Expression,\n",
    "    ProbabilityIntegralTransform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc8780",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"ssp585\"\n",
    "target_name = \"tasmax\"\n",
    "option_2ndfit = False\n",
    "save_files = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99581a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_LAND = 1 / 3\n",
    "REFERENCE_PERIOD = slice(\"1850\", \"1900\")\n",
    "\n",
    "esm = \"IPSL-CM6A-LR\"\n",
    "\n",
    "cmip6_data_path = mesmer.example_data.cmip6_ng_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45deaacd",
   "metadata": {},
   "source": [
    "### Load and prepare predictor and forcing data\n",
    "\n",
    "MESMER-X expectes the predictor and forcing data in the same format as MESMER and MESMER-M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc75b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictor data\n",
    "path_tas = cmip6_data_path / \"tas\" / \"ann\" / \"g025\"\n",
    "\n",
    "fN_hist = path_tas / f\"tas_ann_{esm}_historical_r1i1p1f1_g025.nc\"\n",
    "fN_ssp585 = path_tas / f\"tas_ann_{esm}_{scenario}_r1i1p1f1_g025.nc\"\n",
    "\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n",
    "tas_hist = xr.open_dataset(fN_hist, decode_times=time_coder).drop_vars(\n",
    "    [\"height\", \"file_qf\", \"time_bnds\"]\n",
    ")\n",
    "tas_ssp585 = xr.open_dataset(fN_ssp585, decode_times=time_coder).drop_vars(\n",
    "    [\"height\", \"file_qf\", \"time_bnds\"]\n",
    ")\n",
    "\n",
    "tas = xr.DataTree.from_dict(\n",
    "    {\n",
    "        \"historical\": tas_hist,\n",
    "        \"ssp585\": tas_ssp585,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies w.r.t. the reference period\n",
    "tas_anom = mesmer.anomaly.calc_anomaly(tas, reference_period=REFERENCE_PERIOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e562c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global mean\n",
    "tas_glob_mean = mesmer.weighted.global_mean(tas_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target data\n",
    "path_target = cmip6_data_path / target_name / \"ann\" / \"g025\"\n",
    "\n",
    "fN_hist = path_target / f\"{target_name}_ann_{esm}_historical_r1i1p1f1_g025.nc\"\n",
    "fN_ssp585 = path_target / f\"{target_name}_ann_{esm}_{scenario}_r1i1p1f1_g025.nc\"\n",
    "\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n",
    "targ_hist = xr.open_dataset(fN_hist, decode_times=time_coder)\n",
    "targ_ssp585 = xr.open_dataset(fN_ssp585, decode_times=time_coder)\n",
    "# make sure times align\n",
    "targ_hist[\"time\"] = tas_hist[\"time\"]\n",
    "targ_ssp585[\"time\"] = tas_ssp585[\"time\"]\n",
    "\n",
    "targ_data_orig = xr.DataTree.from_dict(\n",
    "    {\n",
    "        \"historical\": targ_hist,\n",
    "        \"ssp585\": targ_ssp585,\n",
    "    }\n",
    ")\n",
    "\n",
    "targ_data = mesmer.anomaly.calc_anomaly(\n",
    "    targ_data_orig, reference_period=REFERENCE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_stack(ds, threshold_land):\n",
    "    ds = mesmer.mask.mask_ocean_fraction(ds, threshold_land)\n",
    "    ds = mesmer.mask.mask_antarctica(ds)\n",
    "    ds = mesmer.grid.stack_lat_lon(ds, stack_dim=\"gridpoint\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "targ_data = mask_and_stack(targ_data, threshold_land=THRESHOLD_LAND)\n",
    "pred_data = tas_glob_mean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0700557",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = mesmer.datatree.map_over_datasets(xr.ones_like, pred_data)\n",
    "weights = mesmer.datatree.map_over_datasets(\n",
    "    lambda ds: ds.rename({\"tas\": \"weights\"}), weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfae0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "stacked_pred, stacked_targ, stacked_weights = (\n",
    "    mesmer.datatree.broadcast_and_pool_scen_ens(\n",
    "        predictors=pred_data,\n",
    "        target=targ_data,\n",
    "        weights=weights,\n",
    "        member_dim=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3091da6",
   "metadata": {},
   "source": [
    "### Define the Expression and ConditionalDistribution\n",
    "\n",
    "The `Expression` defines the used distribution and the covariate structure and needs to be created as a string and has the following elements:\n",
    "\n",
    "- _distribution_: name of the desired distribution as written in _[scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions)_\n",
    "- _parameters_: the parameters of the distribution as written in scipy (e.g. `loc`, `scale`, ...)\n",
    "- _predictors_: predictors that the distribution will be conditional on, must be written as variable in python and surrounded by `__` (e.g. ``__tas__``)\n",
    "- _coefficients_: of the predictors and parameters, must be named `c1`, `c2`, ..., `c9`.\n",
    "- _mathematical operations_: mathematical operations for the covariates are written as python code. Only functions from numpy (``np.*``) are supported.\n",
    "\n",
    "Thus for the normal distribution with a linear dependence on global mean temperature (_tas_) this looks as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = \"norm(loc=c1 + c2 * __tas__, scale=c3)\"\n",
    "\n",
    "expr_name = \"expr1\"\n",
    "expression_fit = Expression(expr, expr_name, boundaries_params={}, boundaries_coeffs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054be7c",
   "metadata": {},
   "source": [
    "We then pass the `Expression` to the `ConditionalDistribution` class, which finds the best estimate of the coefficients for given climate model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af85a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib = ConditionalDistribution(expression=expression_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acadd567",
   "metadata": {},
   "source": [
    "### Fit conditional distribution\n",
    "\n",
    "Fitting data to arbitrary distributions and covariate structures is a challenging feat - `find_first_guess` uses a multi-step approach to find an inital value for each coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find first guess\n",
    "coeffs_fg = distrib.find_first_guess(\n",
    "    predictors=stacked_pred,\n",
    "    target=stacked_targ.tasmax,\n",
    "    weights=stacked_weights.weights,\n",
    ")\n",
    "coeffs_fg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bba21",
   "metadata": {},
   "source": [
    "The first guess can then be refined by fitting the data using the negative log likelihood as optimization criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the conditional distribution\n",
    "# first round\n",
    "distrib.fit(\n",
    "    predictors=stacked_pred,\n",
    "    target=stacked_targ.tasmax,\n",
    "    weights=stacked_weights.weights,\n",
    "    first_guess=coeffs_fg,\n",
    ")\n",
    "transform_coeffs = distrib.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de58c59",
   "metadata": {},
   "source": [
    "If desired a second round of fit can be done. This approach first calculates a weighted mean of all coefficients in the vicinity of each gridpoint. The weights are inverse to the distance and given by the Gaspari-Cohn function. Please double check if the second round improves your fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second round if necessary\n",
    "if option_2ndfit:\n",
    "    transform_coeffs = distrib.fit(\n",
    "        predictors=stacked_pred,\n",
    "        target=stacked_targ.tasmax,\n",
    "        first_guess=transform_coeffs,\n",
    "        weights=stacked_weights.weights,\n",
    "        sample_dim=\"sample\",\n",
    "        smooth_coeffs=True,\n",
    "        r_gasparicohn=500,\n",
    "    )\n",
    "\n",
    "transform_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d14bb8",
   "metadata": {},
   "source": [
    "### Transform to standard normal distribution\n",
    "\n",
    "After fitting the coefficients the original data is transformed to a standard normal distribution, also removing the fitted linear trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a915c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability integral transform on non-stacked data for AR(1) process\n",
    "target_expression = Expression(\"norm(loc=0, scale=1)\", \"normal_dist\")\n",
    "\n",
    "pit = ProbabilityIntegralTransform(\n",
    "    distrib_orig=distrib,\n",
    "    distrib_targ=ConditionalDistribution(target_expression),\n",
    ")\n",
    "transf_target = pit.transform(\n",
    "    data=targ_data, target_name=target_name, preds_orig=pred_data, preds_targ=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ac834",
   "metadata": {},
   "source": [
    "### Local variability\n",
    "\n",
    "The resulting residuals constitute the local variability and are now treated the same way as in MESMER by fitting a local AR(1) process with a spatially correlated noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20026531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training of auto-regression with spatially correlated innovations\n",
    "local_ar_params = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    transf_target,\n",
    "    ens_dim=\"member\",\n",
    "    dim=\"time\",\n",
    "    lags=1,\n",
    ")\n",
    "\n",
    "# estimate covariance matrix\n",
    "# prep distance matrix\n",
    "geodist = mesmer.geospatial.geodist_exact(\n",
    "    lon=targ_data[\"historical\"].lon, lat=targ_data[\"historical\"].lat\n",
    ")\n",
    "# prep localizer\n",
    "localisation_radii = range(3_500, 15_001, 250)\n",
    "phi_gc_localizer = mesmer.stats.gaspari_cohn_correlation_matrices(\n",
    "    geodist=geodist, localisation_radii=localisation_radii\n",
    ")\n",
    "\n",
    "# NOTE: remove member_dim=None if transf_target contains ensemble members\n",
    "pooled_transf_target = mesmer.datatree.pool_scen_ens(transf_target, member_dim=None)\n",
    "\n",
    "\n",
    "localized_ecov = mesmer.stats.find_localized_empirical_covariance(\n",
    "    data=pooled_transf_target[target_name],\n",
    "    weights=stacked_weights.weights,\n",
    "    localizer=phi_gc_localizer,\n",
    "    dim=\"sample\",\n",
    "    k_folds=30,\n",
    ")\n",
    "\n",
    "localized_ecov[\"localized_covariance_adjusted\"] = mesmer.stats.adjust_covariance_ar1(\n",
    "    localized_ecov.localized_covariance, local_ar_params.coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095640b3",
   "metadata": {},
   "source": [
    "Potentially save the params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = pathlib.Path(\"./output/\")\n",
    "\n",
    "file_end = f\"{target_name}_{expr_name}_{esm}_{scenario}\"\n",
    "distrib_file = test_path / \"distrib\" / f\"params_transform_distrib_{file_end}.nc\"\n",
    "local_ar_file = test_path / \"local_variability\" / f\"params_local_AR_{file_end}.nc\"\n",
    "localized_ecov_file = (\n",
    "    test_path / \"local_variability\" / f\"params_localized_ecov_{file_end}.nc\"\n",
    ")\n",
    "\n",
    "if save_files:\n",
    "    # save the parameters\n",
    "    transform_coeffs.to_netcdf(distrib_file)\n",
    "    local_ar_params.to_netcdf(local_ar_file)\n",
    "    localized_ecov.to_netcdf(localized_ecov_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b1a34",
   "metadata": {},
   "source": [
    "## Make emulations\n",
    "\n",
    "To generate emulations the workflow of the calibration is reversed, using the estimated parameters from above. Here, we use the same local annual mean temperatures to force the emulations, but temperatures from other models, scenarios, ensemble members or emulated annual local temperatures can be used as well.\n",
    "\n",
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some configuration parameters\n",
    "n_realizations = 3\n",
    "seed = 121638161038369005437100875176885634488\n",
    "buffer = 10\n",
    "esm = \"IPSL-CM6A-LR\"\n",
    "\n",
    "targ_var = \"tasmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5de237",
   "metadata": {},
   "source": [
    "Where the seed is random but constant (for reporducibility) and was created using\n",
    "\n",
    "```python\n",
    "import secrets\n",
    "\n",
    "secrets.randbits(64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0459a8c",
   "metadata": {},
   "source": [
    "### Load predictor data\n",
    "\n",
    "In contrast to above we concatenate the historical simulations and the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae031f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_data_path = mesmer.example_data.cmip6_ng_path()\n",
    "\n",
    "# load predictor data\n",
    "path_tas = cmip6_data_path / \"tas\" / \"ann\" / \"g025\"\n",
    "\n",
    "fN_hist = path_tas / f\"tas_ann_{esm}_historical_r1i1p1f1_g025.nc\"\n",
    "fN_ssp585 = path_tas / f\"tas_ann_{esm}_ssp585_r1i1p1f1_g025.nc\"\n",
    "\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n",
    "tas_hist = xr.open_dataset(fN_hist, decode_times=time_coder).drop_vars(\n",
    "    [\"height\", \"file_qf\", \"time_bnds\"]\n",
    ")\n",
    "tas_ssp585 = xr.open_dataset(fN_ssp585, decode_times=time_coder).drop_vars(\n",
    "    [\"height\", \"file_qf\", \"time_bnds\"]\n",
    ")\n",
    "\n",
    "# make global mean\n",
    "# global_mean_dt = map_over_datasets(mesmer.weighted.global_mean)\n",
    "tas_glob_mean_hist = mesmer.weighted.global_mean(tas_hist)\n",
    "tas_glob_mean_ssp585 = mesmer.weighted.global_mean(tas_ssp585)\n",
    "\n",
    "# concat\n",
    "predictor = xr.concat([tas_glob_mean_hist, tas_glob_mean_ssp585], dim=\"time\")\n",
    "\n",
    "predictor = predictor - predictor.sel(time=REFERENCE_PERIOD).mean(\"time\")\n",
    "\n",
    "\n",
    "time = predictor.time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98615383",
   "metadata": {},
   "source": [
    "### Load parameters\n",
    "\n",
    "This is skipped here because they are still defined from the calibration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# test_path = test_data_root_dir / \"output\" / targ_var / \"one_scen_one_ens\"\n",
    "\n",
    "# load the parameters\n",
    "# PARAM_FILEFINDER = FileFinder(\n",
    "#     path_pattern=test_path / \"test-params/{module}/\",\n",
    "#     file_pattern=\"params_{module}_{targ_var}_{expr_name}_{esm}_{scen}.nc\",\n",
    "# )\n",
    "\n",
    "# distrib_file = PARAM_FILEFINDER.find_single_file(\n",
    "#     module=\"distrib\",\n",
    "#     targ_var=targ_var,\n",
    "#     expr_name=expr_name,\n",
    "#     esm=esm,\n",
    "#     scen=scenario,\n",
    "# ).paths.pop()\n",
    "# local_ar_file = PARAM_FILEFINDER.find_single_file(\n",
    "#     module=\"local_trends\",\n",
    "#     targ_var=targ_var,\n",
    "#     expr_name=expr_name,\n",
    "#     esm=esm,\n",
    "#     scen=scenario,\n",
    "# ).paths.pop()\n",
    "# localized_ecov_file = PARAM_FILEFINDER.find_single_file(\n",
    "#     module=\"local_variability\",\n",
    "#     targ_var=targ_var,\n",
    "#     expr_name=expr_name,\n",
    "#     esm=esm,\n",
    "#     scen=scenario,\n",
    "# ).paths.pop()\n",
    "\n",
    "\n",
    "# transform_coeffs = xr.open_dataset(distrib_file)\n",
    "\n",
    "# local_ar_params = xr.open_dataset(local_ar_file)\n",
    "\n",
    "# localized_ecov = xr.open_dataset(localized_ecov_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36137cc9",
   "metadata": {},
   "source": [
    "### Create temporally and spatially corelated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate realizations based on the auto-regression with spatially correlated innovations\n",
    "transf_emus = mesmer.stats.draw_auto_regression_correlated(\n",
    "    local_ar_params,\n",
    "    localized_ecov.localized_covariance_adjusted,\n",
    "    time=time,\n",
    "    realisation=n_realizations,\n",
    "    seed=seed,\n",
    "    buffer=buffer,\n",
    ")\n",
    "transf_emus = transf_emus.rename({\"samples\": targ_var})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae84ce7",
   "metadata": {},
   "source": [
    "### Transform samples back to the conditional distribution\n",
    "\n",
    "Initialize the conditional distribution with the calibrated values and the expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd43431",
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_orig = ConditionalDistribution.from_dataset(transform_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd23f63",
   "metadata": {},
   "source": [
    "define a standard normal distribution and the probability integral transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96378df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-transform the realizations\n",
    "expr_tranf = Expression(\"norm(loc=0, scale=1)\", \"standard_normal\")\n",
    "distrib_transf = ConditionalDistribution(expr_tranf)\n",
    "\n",
    "back_pit = ProbabilityIntegralTransform(\n",
    "    distrib_orig=distrib_transf,\n",
    "    distrib_targ=distrib_orig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f3deb",
   "metadata": {},
   "source": [
    "and fially back-transform the realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emus = back_pit.transform(\n",
    "    data=transf_emus, target_name=targ_var, preds_orig=None, preds_targ=predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857f69b",
   "metadata": {},
   "source": [
    "### Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e116af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridpoint = 0\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "emus.tasmax.isel(gridpoint=gridpoint).plot.line(\n",
    "    ax=ax, x=\"time\", color=\"#6baed6\", add_legend=False\n",
    ")\n",
    "\n",
    "ax.plot([], [], label=\"realisations\", color=\"#6baed6\")\n",
    "\n",
    "stacked_targ.tasmax.isel(gridpoint=gridpoint).plot(\n",
    "    ax=ax, x=\"time\", color=\"0.1\", label=\"training data\", lw=1\n",
    ")\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(\"Training data and emulations at random grid point\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesmer_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
