{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MESMER Tutorial\n",
    "This is a tutorial for the use of MESMER. Here we demonstrate how MESMER is able to produce Earth System Model-specific spatio-temporally correlated temperature field realizations, taking annual global mean near surface temperature as input and emulating annual local near surface land temperature. This tutorial shows how to calibrate the parameters for MESMER on an example dataset of coarse regridded ESM output and emulate new realisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from filefisher import FileFinder\n",
    "\n",
    "import mesmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating MESMER\n",
    "\n",
    "We calibrate the parameters for MESMER using three scenarios: a historical, a low emission (SSP1-2.6), and a high emission (SSP5-8.5) scenario, each including several ensemble members. Training MESMER consists of four parts:\n",
    "\n",
    "- **global trend**: compute the global temperature trend, including the volcanic influence on historical trends\n",
    "- **global variablity**: estimating the parameters to generate global variability\n",
    "- **local trend**: estimate parameters to translate global mean temperature (including global variability) into locally resolved temperature\n",
    "- **local variability**: estimate parameters needed to generate local variability\n",
    "\n",
    "You can find the basics of this approach in: Beusch, et al. ([2020](https://doi.org/10.5194/ESD-11-139-2020)): \"*Emulating Earth system model temperatures with MESMER: From global mean temperature trajectories to grid-point-level realizations on land.*\" Earth System Dynamics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMSER expects a specific data format. Data from each scenario should be node (or group) on an `xr.DataTree` (more on this below) e.g.:\n",
    "\n",
    "```\n",
    "<xarray.DataTree>\n",
    "Group: /\n",
    "├── Group: /historical\n",
    "|    ...\n",
    "├── Group: /ssp126\n",
    "|    ...\n",
    "```\n",
    "\n",
    "Each node should be `xr.Dataset` with 4 dimensions: member, time, lat, lon. Below we show one way to load data such that it conforms to the desired format. We load data from the cmip6-ng (\"new generation\") repository. This data has undergone a small reformatting from the original cmip6 archive. For the sake of computational speed we also load data which has been regridded to a coarse resolution. Loading the data can be adapted to the data format you are most used to - as long as the final output has the desired format.\n",
    "\n",
    "---\n",
    "\n",
    "MESMER is Earth System Model specific, aiming to reproduce to some extent the behaviour of one ESM. Here we train on the CMIP6 output of the model IPSL-CM6A-LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"IPSL-CM6A-LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the library [filefisher](https://github.com/mpytools/filefisher) to search all files in the cmip6-ng archive for the model and scenarios we want to use. Filefisher can search through paths for given certain file patterns. It returns all paths matching the pattern such that you can load the files in the next step.\n",
    "\n",
    "Here, we want to find all files that have data for annual near surface temperature (tas) for the used model and the future scenarios ssp126 and ssp585. Next, we search for the historical data that match the members found for the two future scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip_path = mesmer.example_data.cmip6_ng_path(relative=True)\n",
    "\n",
    "CMIP_FILEFINDER = FileFinder(\n",
    "    path_pattern=cmip_path / \"{variable}/{time_res}/{resolution}\",\n",
    "    file_pattern=\"{variable}_{time_res}_{model}_{scenario}_{member}_{resolution}.nc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search data for ssp126 and ssp585 - we find one and two ensemble members, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"ssp126\", \"ssp585\"]\n",
    "\n",
    "fc_scens = CMIP_FILEFINDER.find_files(\n",
    "    variable=\"tas\", scenario=scenarios, model=model, resolution=\"g025\", time_res=\"ann\"\n",
    ")\n",
    "\n",
    "\n",
    "fc_scens.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to find the same ensemble members in the historical data, such that we end with five files we need to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the historical members that are also in the future scenarios, but only once\n",
    "members = fc_scens.df.member.unique()\n",
    "\n",
    "fc_hist = CMIP_FILEFINDER.find_files(\n",
    "    variable=\"tas\",\n",
    "    scenario=\"historical\",\n",
    "    model=model,\n",
    "    resolution=\"g025\",\n",
    "    time_res=\"ann\",\n",
    "    member=members,\n",
    ")\n",
    "\n",
    "fc_all = fc_hist.concat(fc_scens)\n",
    "fc_all.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load all the files we found into a ``DataTree``. ``DataTree`` is a data structure provided by [xarray](https://docs.xarray.dev/en/stable/index.html).\n",
    "\n",
    "Essentially, ``DataTree`` is a container to hold xarray `Ddataset` with data variables that are not alignable. This is useful for us since we have historical and future data, which have different time coordinates. Moreover, the scenarios may also have different numbers of members (as e.g. ssp126, which only has one). Thus, we store the data of each sceanrio in a `xarray.Dataset` holding all its ensemble members along a `member` dimension. Then we store all the scenario datasets in one `DataTree`. The `DataTree` allows us to perform computations on each of the datasets in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = xr.DataTree()\n",
    "\n",
    "scenarios_incl_hist = [\"historical\"] + scenarios\n",
    "\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n",
    "\n",
    "# load data for each scenario\n",
    "for scen in scenarios_incl_hist:\n",
    "    files = fc_all.search(scenario=scen)\n",
    "\n",
    "    # load all members for a scenario\n",
    "    members = []\n",
    "    for fN, meta in files.items():\n",
    "        ds = xr.open_dataset(fN, decode_times=time_coder)\n",
    "        # drop unnecessary variables\n",
    "        ds = ds.drop_vars([\"height\", \"time_bnds\", \"file_qf\"], errors=\"ignore\")\n",
    "        # assign member-ID as coordinate\n",
    "        ds = ds.assign_coords({\"member\": meta[\"member\"]})\n",
    "        members.append(ds)\n",
    "\n",
    "    # create a Dataset that holds each member along the member dimension\n",
    "    scen_data = xr.concat(members, dim=\"member\")\n",
    "    # put the scenario dataset into the DataTree\n",
    "    dt[scen] = xr.DataTree(scen_data)\n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in the data format discussed above. You can examine it by clicking on `Groups` above.\n",
    "\n",
    "---\n",
    "\n",
    "We will need some configuration parameters in the follwing:\n",
    "1. ``THRESHOLD_LAND``: Threshold above which land fraction to consider a grid point as a land grid point.\n",
    "2. ``REFERENCE_PERIOD``: We will work not with abolute temperature values but with temperature anomalies w.r.t. a reference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_LAND = 1 / 3\n",
    "REFERENCE_PERIOD = slice(\"1850\", \"1900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies w.r.t. the reference period\n",
    "tas_anom = mesmer.anomaly.calc_anomaly(dt, reference_period=REFERENCE_PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volcanic contributions\n",
    "The volcanic contributions to the global mean temperature trend of the historical period have to be removed in order to estimate the linear regression of global mean temperature to local temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate global mean and smooth the forcing data using a lowess smoother, using 50 time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global mean\n",
    "tas_globmean = mesmer.weighted.global_mean(tas_anom)\n",
    "tas_globmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean over members before smoothing\n",
    "\n",
    "tas_globmean_ensmean = tas_globmean.mean(dim=\"member\")\n",
    "\n",
    "n_steps = 50\n",
    "tas_globmean_smoothed = mesmer.stats.lowess(\n",
    "    tas_globmean_ensmean,\n",
    "    dim=\"time\",\n",
    "    n_steps=n_steps,\n",
    "    use_coords=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "h0, *_ = tas_globmean[\"historical\"].tas.plot.line(ax=ax, x=\"time\", color=\"grey\", lw=1)\n",
    "a2, *_ = tas_globmean_smoothed[\"historical\"].tas.plot.line(ax=ax, x=\"time\", lw=2)\n",
    "\n",
    "ax.legend([h0, a2], [\"Ensemble members\", \"Smooth ensemble mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit the parameter of the volcanic contributions only on the historical smoothed data of all ensemble members available. The future scenarios do not have volcanic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_tas_residuals = tas_globmean[\"historical\"] - tas_globmean_smoothed[\"historical\"]\n",
    "\n",
    "# fit volcanic influence\n",
    "volcanic_params = mesmer.volc.fit_volcanic_influence(hist_tas_residuals.tas)\n",
    "\n",
    "volcanic_params.aod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Superimpose the volcanic influence on the historical time series. Note that due to the approach to handle the historical data as its own scenario, we ecounter discontinuities at the boundary between historical and future period. However, this is not relevant for the fitting of the parameters hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superimpose the volcanic forcing on historical data\n",
    "tas_globmean_smoothed[\"historical\"] = mesmer.volc.superimpose_volcanic_influence(\n",
    "    tas_globmean_smoothed[\"historical\"],\n",
    "    volcanic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plotting\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "# plot unsmoothed global means\n",
    "tas_globmean[\"historical\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"grey\", add_legend=False\n",
    ")\n",
    "tas_globmean[\"ssp126\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"#6baed6\", add_legend=False\n",
    ")\n",
    "tas_globmean[\"ssp585\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"pink\", add_legend=False\n",
    ")\n",
    "\n",
    "# plot smoothed global means including volcanic influence for historical\n",
    "tas_globmean_smoothed[\"historical\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"black\", label=\"historical\"\n",
    ")\n",
    "tas_globmean_smoothed[\"ssp126\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"blue\", label=\"ssp126\"\n",
    ")\n",
    "tas_globmean_smoothed[\"ssp585\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"red\", label=\"ssp585\"\n",
    ")\n",
    "\n",
    "\n",
    "histend = tas_globmean[\"historical\"].time.isel(time=-1).item()\n",
    "ax.axvline(histend, color=\"0.4\")\n",
    "ax.axhline(0, color=\"0.1\", lw=0.5)\n",
    "\n",
    "ax.set_title(\"\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate residuals (w.r.t. smoothed ts) i.e. remove the smoothed global mean, including the volcanic influence from the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_globmean_resids = tas_globmean - tas_globmean_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "h0, *_ = tas_globmean_resids[\"historical\"].tas.plot.line(\n",
    "    x=\"time\", color=\"grey\", lw=1, add_legend=False\n",
    ")\n",
    "h1, *_ = tas_globmean_resids[\"ssp126\"].tas.plot.line(\n",
    "    x=\"time\", color=\"blue\", lw=1, add_legend=False\n",
    ")\n",
    "h2, *_ = tas_globmean_resids[\"ssp585\"].tas.plot.line(\n",
    "    x=\"time\", color=\"red\", lw=1, add_legend=False\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Residuals\")\n",
    "plt.axhline(0, lw=1, color=\"0.1\")\n",
    "histend = tas_globmean_resids[\"historical\"].time.isel(time=-1).item()\n",
    "plt.axvline(histend, color=\"0.1\", lw=2)\n",
    "\n",
    "plt.legend([h0, h1, h2], [\"historical\", \"ssp126\", \"ssp585\"])\n",
    "plt.show()\n",
    "# TODO add legend for the scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to fit an AR process for estimating global variability, taking in the residual global mean temperature as follows:\n",
    "\n",
    "$T_{t}^{glob, var} = \\alpha_0 + \\sum\\limits_{k=1}^{k=p} \\alpha_k \\cdot T_{t-k}^{glob, var} + \\varepsilon_t,\\ \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the order of the AR process and then fit the parameters. Internally, we fit the parameters for each member and then average first over the parameters of each scenario and then over all scenarios to arrive at a single set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_order = mesmer.stats.select_ar_order_scen_ens(\n",
    "    tas_globmean_resids, dim=\"time\", ens_dim=\"member\", maxlag=12, ic=\"bic\"\n",
    ")\n",
    "\n",
    "global_ar_params = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    tas_globmean_resids, dim=\"time\", ens_dim=\"member\", lags=ar_order\n",
    ")\n",
    "\n",
    "global_ar_params.drop_vars(\"nobs\")\n",
    "# TODO: drop nobs internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local forced response\n",
    "Now we need to estimate how the global trend translates into a local forced response. This is done using a linear regression of the global trend and the global variability as predictors:\n",
    "\n",
    "$T_{s,t}^{resp} = \\beta_s^{trend} \\cdot T_t^{glob, trend} + \\beta_s^{int} + \\beta_s^{var} \\cdot T_t^{glob, var}$\n",
    "\n",
    "To this end, we stack all values (members, scenarios) into a single dataset, the only important thing is that predictor and predicted values stay together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare the local temperature data:\n",
    "\n",
    "1. Mask out ocean grid points (where the land fraction is larger than `THRESHOLD_LAND`)\n",
    "2. Mask out Antarctica\n",
    "3. Convert the data from a 2D lat-lon grid to a 1D grid by stacking it and removing all gridcells that were previously masked out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before stacking, we extract the original grid. We need to save this together with the parameters to later be able to reconstruct the original grid from the gridpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract original grid\n",
    "grid_orig = tas_anom[\"historical\"].ds[[\"lat\", \"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_stack(dt, threshold_land):\n",
    "    dt = mesmer.mask.mask_ocean_fraction(dt, threshold_land)\n",
    "    dt = mesmer.mask.mask_antarctica(dt)\n",
    "    dt = mesmer.grid.stack_lat_lon(dt)\n",
    "    return dt\n",
    "\n",
    "\n",
    "# mask and stack the data\n",
    "tas_stacked = mask_and_stack(tas_anom, THRESHOLD_LAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_stacked[\"ssp585\"].tas.isel(member=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now converted the 3D field (with dimensions lat, lon, and time) to a 2D field (with dimensions gridcell and time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weights\n",
    "weights = mesmer.weighted.equal_scenario_weights_from_datatree(tas_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `DataTree` from all predictors - here the smoothed global mean and it's residuals. We could add more predictors similarly (e.g. the squared temperatures or the ocean heat uptake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = xr.DataTree.from_dict(\n",
    "    {\"tas\": tas_globmean_smoothed, \"tas_resids\": tas_globmean_resids}\n",
    ")\n",
    "# predictors[\"tas2\"] = tas_globmean_smoothed ** 2\n",
    "\n",
    "target = tas_stacked.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimated the forced response use a linear regression. For this we need to stack the different scenarios and ensemble members for the predictors, target, and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update after #633\n",
    "predictors_stacked, target_stacked, weights_stacked = (\n",
    "    mesmer.datatree.stack_datatrees_for_linear_regression(\n",
    "        predictors, target, weights, stacking_dims=[\"member\", \"time\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "target_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus insead now the `DataTree` over the scenario, and the time and ensemble dimension are stacked to the sample dimension. We can now fit these using a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_lin_reg = mesmer.stats.LinearRegression()\n",
    "\n",
    "local_lin_reg.fit(\n",
    "    predictors=predictors_stacked,\n",
    "    target=target_stacked.tas,\n",
    "    dim=\"sample\",\n",
    "    weights=weights_stacked.weights,\n",
    ")\n",
    "\n",
    "local_forced_response_params = local_lin_reg.params\n",
    "local_forced_response_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars = (\n",
    "    \"intercept\",\n",
    "    \"tas\",\n",
    "    \"tas_resids\",\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(\n",
    "    3, 1, sharex=True, sharey=True, subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    ")\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, data_var in zip(axs, data_vars):\n",
    "\n",
    "    da = local_forced_response_params[data_var]\n",
    "    da = mesmer.grid.unstack_lat_lon_and_align(da, grid_orig)\n",
    "\n",
    "    h = da.plot(\n",
    "        ax=ax, label=data_var, robust=True, center=0, extend=\"both\", add_colorbar=False\n",
    "    )\n",
    "\n",
    "    ax.set_extent((-180, 180, -60, 85), ccrs.PlateCarree())\n",
    "    cbar = plt.colorbar(h, ax=ax, extend=\"both\", pad=0.025)  # , shrink=0.7)\n",
    "    ax.set(title=data_var, xlabel=\"\", ylabel=\"\", xticks=[], yticks=[])\n",
    "    ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local variability\n",
    "Now we need to fit the parameters for the AR(1) process with a spatially correlated noise term used to emulate local variability:\n",
    "\n",
    "$\\eta_{s,t} = \\gamma_{0, s} + \\gamma_{1, 2} \\cdot \\eta_{s, t-1} + \\nu_{s,t}, \\ \\nu_{s,t} \\sim \\mathcal{N}(0, \\Sigma_{\\nu}(r))$\n",
    "\n",
    "The first component that contains the AR parameters ensures temporal correlation of the local variability whereas the noise term ensures spatial consistency. The covariance matrix is estimated on the whole grid and represents the spatial correlation of temperatures at the different gridpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the AR parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to compute the residuals after the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_stacked_residuals = local_lin_reg.residuals(\n",
    "    predictors=predictors_stacked, target=target_stacked.tas\n",
    ").T\n",
    "\n",
    "tas_stacked_residuals.plot()\n",
    "\n",
    "# unstack the residuals\n",
    "tas_un_stacked_residuals = tas_stacked_residuals.set_index(\n",
    "    sample=(\"time\", \"member\", \"scenario\")\n",
    ").unstack(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put each scenario into a datatree again, get rid of superfluous time steps and/or members\n",
    "dt_resids = xr.DataTree()\n",
    "for scenario in tas_un_stacked_residuals.scenario.values:\n",
    "    dt_resids[scenario] = xr.DataTree(\n",
    "        tas_un_stacked_residuals.to_dataset()\n",
    "        .sel(scenario=scenario)\n",
    "        .dropna(\"member\", how=\"all\")\n",
    "        .dropna(\"time\")\n",
    "        .drop_vars(\"scenario\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ar = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    dt_resids,\n",
    "    ens_dim=\"member\",\n",
    "    dim=\"time\",\n",
    "    lags=1,\n",
    ")\n",
    "\n",
    "local_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate covariance matrix\n",
    "For the covariance matrix of the white noise we first estimate the empirical covariance matrix of the gridcell's values and then localize it using the Gaspari-Cohn function. This function goes to 0 for for distances bigger than the so called localisation radius. This is also called regularization. It ensures that grid points that are further away from each other do not correlate. Such spurious correlations can arise from rank deficient covariance matrices. In our case because we estimate the covariance on data that has more gridcells than timesteps.\n",
    "\n",
    "The localisation radius is a parameter that needs to be calibrated and we find the best localisation radius by cross-validation of several radii using the negative loglikelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the distance matrix - the distance between the gridpoints in km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt_resids[\"historical\"].ds\n",
    "geodist = mesmer.geospatial.geodist_exact(dt.lon, dt.lat)\n",
    "\n",
    "# plot\n",
    "f, ax = plt.subplots()\n",
    "geodist.plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. prepare the localizer(s) to regularize the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_gc_localizer = mesmer.stats.gaspari_cohn_correlation_matrices(\n",
    "    geodist, range(5_000, 15_001, 500)\n",
    ")\n",
    "\n",
    "# plot one\n",
    "f, ax = plt.subplots()\n",
    "phi_gc_localizer[5000].plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing weights from local trend regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. find the best localization radius and localize the empirical covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = \"sample\"\n",
    "k_folds = 15\n",
    "\n",
    "localized_ecov = mesmer.stats.find_localized_empirical_covariance(\n",
    "    tas_stacked_residuals, weights_stacked.weights, phi_gc_localizer, dim, k_folds\n",
    ")\n",
    "\n",
    "localized_ecov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, sharey=True, constrained_layout=True)\n",
    "\n",
    "opt = dict(vmin=0, vmax=1.5, cmap=\"Blues\", add_colorbar=False)\n",
    "\n",
    "ax = axs[0]\n",
    "localized_ecov.covariance.plot(ax=ax, **opt)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Empirical covariance\")\n",
    "\n",
    "ax = axs[1]\n",
    "localized_ecov.localized_covariance.plot(ax=ax, **opt)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Localized empirical covariance\")\n",
    "ax.set_ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Adjust the regularized covariance matrix\n",
    "\n",
    "Lastly we need to adjust the localized covariance matrix using the AR(1) parameters since the variance of the time series we observe is bigger than the variance of the driving white noise process. Read more about this here: \"Statistical Analysis in Climate Research\" by Stroch and Zwiers (1999, reprinted 2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localized_covariance_adjusted = mesmer.stats.adjust_covariance_ar1(\n",
    "    localized_ecov.localized_covariance, local_ar.coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have calibrated all needed parameters and can save them. We can use filefisher to nicely create file names and save the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path relative to this notebook & create folder\n",
    "param_path = pathlib.Path(\"./output/tas/multi_scen_multi_ens\")\n",
    "param_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_FILEFINDER = FileFinder(\n",
    "    path_pattern=param_path / \"test-params/{module}/\",\n",
    "    file_pattern=\"params_{module}_{esm}_{scen}.nc\",\n",
    ")\n",
    "\n",
    "scen_str = \"-\".join(scenarios)\n",
    "\n",
    "volcanic_file = PARAM_FILEFINDER.create_full_name(\n",
    "    module=\"volcanic\",\n",
    "    esm=model,\n",
    "    scen=scen_str,\n",
    ")\n",
    "global_ar_file = PARAM_FILEFINDER.create_full_name(\n",
    "    module=\"global-variability\",\n",
    "    esm=model,\n",
    "    scen=scen_str,\n",
    ")\n",
    "local_forced_file = PARAM_FILEFINDER.create_full_name(\n",
    "    module=\"local-trends\",\n",
    "    esm=model,\n",
    "    scen=scen_str,\n",
    ")\n",
    "local_ar_file = PARAM_FILEFINDER.create_full_name(\n",
    "    module=\"local-variability\",\n",
    "    esm=model,\n",
    "    scen=scen_str,\n",
    ")\n",
    "localized_ecov_file = PARAM_FILEFINDER.create_full_name(\n",
    "    module=\"covariance\",\n",
    "    esm=model,\n",
    "    scen=scen_str,\n",
    ")\n",
    "\n",
    "save_files = False  # we don't save them here in the example\n",
    "if save_files:\n",
    "    # save the parameters\n",
    "    volcanic_params.to_netcdf(volcanic_file)\n",
    "    global_ar_params.to_netcdf(global_ar_file)\n",
    "    local_lin_reg.to_netcdf(local_forced_file)\n",
    "    local_ar.to_netcdf(local_ar_file)\n",
    "    localized_ecov.to_netcdf(localized_ecov_file)\n",
    "\n",
    "# TODO: save the original grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear everything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emulating near surface temperature on land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from filefisher import FileFinder\n",
    "\n",
    "import mesmer\n",
    "from mesmer.core._datatreecompat import map_over_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load forcing data\n",
    "Now one can use any global mean temperature trajectory to draw gridded realisations. For this example we want to create emulations for ssp126 and ssp585 to compare the emulations to the actual ESM output. Here we concatenate historical and future runs to create a continuous timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"IPSL-CM6A-LR\"\n",
    "scenarios = [\"ssp126\", \"ssp585\"]\n",
    "\n",
    "# some configuration parameters\n",
    "THRESHOLD_LAND = 1 / 3\n",
    "\n",
    "REFERENCE_PERIOD = slice(\"1850\", \"1900\")\n",
    "\n",
    "HIST_PERIOD = slice(\"1850\", \"2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the files - use same relative path as above\n",
    "param_path = pathlib.Path(\"./output/tas/multi_scen_multi_ens\")\n",
    "\n",
    "CMIP_FILEFINDER = FileFinder(\n",
    "    path_pattern=cmip_path / \"{variable}/{time_res}/{resolution}\",\n",
    "    file_pattern=\"{variable}_{time_res}_{model}_{scenario}_{member}_{resolution}.nc\",\n",
    ")\n",
    "\n",
    "fc_scens = CMIP_FILEFINDER.find_files(\n",
    "    variable=\"tas\", scenario=scenarios, model=model, resolution=\"g025\", time_res=\"ann\"\n",
    ")\n",
    "\n",
    "members = fc_scens.df.member.unique()\n",
    "\n",
    "fc_hist = CMIP_FILEFINDER.find_files(\n",
    "    variable=\"tas\",\n",
    "    scenario=\"historical\",\n",
    "    model=model,\n",
    "    resolution=\"g025\",\n",
    "    time_res=\"ann\",\n",
    "    member=members,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hist(meta, fc_hist):\n",
    "\n",
    "    meta_hist = meta | {\"scenario\": \"historical\"}\n",
    "\n",
    "    fc = fc_hist.search(**meta_hist)\n",
    "\n",
    "    if len(fc) == 0:\n",
    "        raise FileNotFoundError(\"no hist file found\")\n",
    "    if len(fc) != 1:\n",
    "        raise ValueError(\"more than one hist file found\")\n",
    "\n",
    "    fN, meta_hist = fc[0]\n",
    "\n",
    "    return fN, meta_hist\n",
    "\n",
    "\n",
    "def load_hist(meta, fc_hist):\n",
    "    fN, __ = _get_hist(meta, fc_hist)\n",
    "    return xr.open_dataset(fN, use_cftime=True)\n",
    "\n",
    "\n",
    "def load_hist_scen_continuous(fc_hist, fc_scens):\n",
    "    dt = xr.DataTree()\n",
    "    for scen in fc_scens.df.scenario.unique():\n",
    "        files = fc_scens.search(scenario=scen)\n",
    "\n",
    "        members = []\n",
    "\n",
    "        for fN, meta in files.items():\n",
    "\n",
    "            try:\n",
    "                hist = load_hist(meta, fc_hist)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            proj = xr.open_dataset(fN, use_cftime=True)\n",
    "\n",
    "            ds = xr.combine_by_coords(\n",
    "                [hist, proj],\n",
    "                combine_attrs=\"override\",\n",
    "                data_vars=\"minimal\",\n",
    "                compat=\"override\",\n",
    "                coords=\"minimal\",\n",
    "            )\n",
    "\n",
    "            ds = ds.drop_vars((\"height\", \"time_bnds\", \"file_qf\"), errors=\"ignore\")\n",
    "\n",
    "            ds = mesmer.grid.wrap_to_180(ds)\n",
    "\n",
    "            # assign member-ID as coordinate\n",
    "            ds = ds.assign_coords({\"member\": meta[\"member\"]})\n",
    "\n",
    "            members.append(ds)\n",
    "\n",
    "        # create a Dataset that holds each member along the member dimension\n",
    "        scen_data = xr.concat(members, dim=\"member\")\n",
    "        # put the scenario dataset into the DataTree\n",
    "        dt[scen] = xr.DataTree(scen_data)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = load_hist_scen_continuous(fc_hist, fc_scens)\n",
    "ref = tas.sel(time=REFERENCE_PERIOD).mean(\"time\")\n",
    "tas_anom = tas - ref\n",
    "tas_globmean = mesmer.weighted.global_mean(tas_anom)\n",
    "\n",
    "tas_globmean_ensmean = tas_globmean.mean(dim=\"member\")\n",
    "tas_globmean_forcing = mesmer.stats.lowess(\n",
    "    tas_globmean_ensmean,\n",
    "    dim=\"time\",\n",
    "    n_steps=30,\n",
    "    use_coords=False,\n",
    ")\n",
    "time = tas_globmean_forcing[\"ssp126\"].time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_path = data_path / \"output\" / \"tas\" / \"multi_scen_multi_ens\"\n",
    "\n",
    "PARAM_FILEFINDER = FileFinder(\n",
    "    path_pattern=param_path / \"test-params/{module}/\",\n",
    "    file_pattern=\"params_{module}_{esm}_{scen}.nc\",\n",
    ")\n",
    "scen_str = \"-\".join(scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modules = [\n",
    "    \"volcanic\",\n",
    "    \"global-variability\",\n",
    "    \"local-trends\",\n",
    "    \"local-variability\",\n",
    "    \"covariance\",\n",
    "]\n",
    "param_files = PARAM_FILEFINDER.find_files(module=all_modules, esm=model, scen=scen_str)\n",
    "\n",
    "params = xr.DataTree()\n",
    "\n",
    "for module in all_modules:\n",
    "    params[module] = xr.DataTree(\n",
    "        xr.open_dataset(param_files.search(module=module).paths.pop()), name=module\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define seeds for global and local variability \n",
    "If we want reproducible results we need to set a seed for the random samples of global and local variability. Here, we set the seed to a chosen number, but for automated generation of seeds i.e. for several ESM we recommend using the `secrets` from the standard library. \n",
    "Then you would generate a seed using:\n",
    "\n",
    "```python\n",
    "import secrets\n",
    "\n",
    "xr.Dataset(data_vars={\"seed\": secrets.randbits(64)})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_global_variability = xr.DataTree.from_dict(\n",
    "    {\n",
    "        \"ssp126\": xr.Dataset(data_vars={\"seed\": 981}),\n",
    "        \"ssp585\": xr.Dataset(data_vars={\"seed\": 314}),\n",
    "    }\n",
    ")\n",
    "seed_local_variability = xr.DataTree.from_dict(\n",
    "    {\n",
    "        \"ssp126\": xr.Dataset(data_vars={\"seed\": 272}),\n",
    "        \"ssp585\": xr.Dataset(data_vars={\"seed\": 42}),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make emulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some settings\n",
    "n_realisations = 10\n",
    "\n",
    "buffer_global_variability = 50\n",
    "buffer_local_variability = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Adding the volcanic influence to the smooth global mean forcing\n",
    "This is optional, depending on if you want to reproduce the past accurately. This is necessary when we want to evaluate the performance of our emulator on ESM or observation data but might not be necessary for more abstract research questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_globmean_forcing = mesmer.volc.superimpose_volcanic_influence(\n",
    "    tas_globmean_forcing,\n",
    "    params[\"volcanic\"].ds,\n",
    "    hist_period=HIST_PERIOD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_globmean_forcing[\"ssp126\"].to_dataset().tas.plot()\n",
    "tas_globmean_forcing[\"ssp585\"].to_dataset().tas.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute global variabilty \n",
    "Draw samples from a AR process with the calibrated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_variability = mesmer.stats.draw_auto_regression_uncorrelated(\n",
    "    params[\"global-variability\"].ds,\n",
    "    realisation=n_realisations,\n",
    "    time=time,\n",
    "    seed=seed_global_variability,\n",
    "    buffer=buffer_global_variability,\n",
    ")\n",
    "global_variability = map_over_datasets(\n",
    "    lambda ds: ds.rename({\"samples\": \"tas\"}), global_variability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute local forced response\n",
    "Apply linear regression using the global mean forcing and the global variability as predictors. Optionally, you can also add other variables to the predictors like ocean heat content or squared global mean temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = xr.DataTree.from_dict(\n",
    "    {\n",
    "        \"ssp126\": xr.DataTree.from_dict(\n",
    "            {\n",
    "                \"tas\": tas_globmean_forcing[\"ssp126\"],\n",
    "                \"tas_resids\": global_variability[\"ssp126\"],\n",
    "            }\n",
    "        ),\n",
    "        \"ssp585\": xr.DataTree.from_dict(\n",
    "            {\n",
    "                \"tas\": tas_globmean_forcing[\"ssp585\"],\n",
    "                \"tas_resids\": global_variability[\"ssp585\"],\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "lr = mesmer.stats.LinearRegression()\n",
    "lr.params = params[\"local-trends\"].ds\n",
    "\n",
    "# uses ``exclude`` to split the linear response\n",
    "local_forced_response = xr.DataTree()\n",
    "local_variability_from_global_var = xr.DataTree()\n",
    "\n",
    "for scen in predictors.children:\n",
    "    # local variability part driven by global mean\n",
    "    local_forced_response[scen] = xr.DataTree(\n",
    "        lr.predict(predictors[scen], exclude={\"tas_resids\"}).rename(\"tas\").to_dataset()\n",
    "    )\n",
    "\n",
    "    # local variability part driven by global variabilty\n",
    "    local_variability_from_global_var[scen] = xr.DataTree(\n",
    "        lr.predict(predictors[scen], exclude={\"tas\", \"intercept\"})\n",
    "        .rename(\"tas\")\n",
    "        .to_dataset()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute local variability\n",
    "We compute the local variability by applying an AR(1) process to ensure consistency in time and adding spatially correlated innovations at each time step to get spatially coherent random samples at each gridpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_variability = mesmer.stats.draw_auto_regression_correlated(\n",
    "    params[\"local-variability\"].ds,\n",
    "    params[\"covariance\"].localized_covariance,\n",
    "    time=time,\n",
    "    realisation=n_realisations,\n",
    "    seed=seed_local_variability,\n",
    "    buffer=buffer_local_variability,\n",
    ")\n",
    "local_variability = map_over_datasets(\n",
    "    lambda ds: ds.rename({\"samples\": \"tas\"}), local_variability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Add everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_variability_total = local_variability_from_global_var + local_variability\n",
    "emulations = local_forced_response + local_variability_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving emulations\n",
    "We recommend saving the emulations together with the seeds used for emulating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scen in emulations:\n",
    "    local_seed = seed_local_variability[scen].seed.rename(\"seed_local_variability\")\n",
    "    global_seed = seed_global_variability[scen].seed.rename(\"seed_global_variability\")\n",
    "    emulations[scen] = xr.DataTree(\n",
    "        xr.merge([emulations[scen].ds, local_seed, global_seed])\n",
    "    )\n",
    "\n",
    "emu_path = data_path / \"output\" / \"tas\" / \"multi_scen_multi_ens\"\n",
    "# emulations.to_netcdf(emu_path / f\"emulations_{model}_ssp126-ssp585.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_orig = tas_anom[\"ssp126\"].ds[[\"lat\", \"lon\"]]\n",
    "spatial_emu_126 = mesmer.grid.unstack_lat_lon_and_align(\n",
    "    emulations[\"ssp126\"].tas, grid_orig\n",
    ")\n",
    "spatial_emu_585 = mesmer.grid.unstack_lat_lon_and_align(\n",
    "    emulations[\"ssp585\"].tas, grid_orig\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(3, 1, subplot_kw={\"projection\": ccrs.Robinson()})\n",
    "\n",
    "opt = dict(cmap=\"Reds\", transform=ccrs.PlateCarree(), vmin=0, vmax=15, extend=\"max\")\n",
    "spatial_emu_126.mean(\"realisation\").sel(time=\"2100\").plot(ax=axs[0], **opt)\n",
    "spatial_emu_585.mean(\"realisation\").sel(time=\"2100\").plot(ax=axs[1], **opt)\n",
    "\n",
    "diff = spatial_emu_585 - spatial_emu_126\n",
    "diff.mean(\"realisation\").sel(time=\"2100\").plot(\n",
    "    ax=axs[2], cmap=\"RdBu_r\", transform=ccrs.PlateCarree(), center=0\n",
    ")\n",
    "\n",
    "axs[0].set_title(\"ssp126 2100\")\n",
    "axs[1].set_title(\"ssp585 2100\")\n",
    "axs[2].set_title(\"Difference\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.coastlines()\n",
    "    ax.set_global()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot global means\n",
    "globmean_126 = mesmer.weighted.global_mean(spatial_emu_126)\n",
    "globmean_585 = mesmer.weighted.global_mean(spatial_emu_585)\n",
    "\n",
    "globmean_126_smoothed = mesmer.stats.lowess(\n",
    "    globmean_126.mean(\"realisation\"), dim=\"time\", n_steps=50, use_coords=False\n",
    ")\n",
    "globmean_585_smoothed = mesmer.stats.lowess(\n",
    "    globmean_585.mean(\"realisation\"), dim=\"time\", n_steps=50, use_coords=False\n",
    ")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "globmean_585.plot.line(x=\"time\", ax=ax, add_legend=False, color=\"lightblue\")\n",
    "globmean_126.plot.line(x=\"time\", ax=ax, add_legend=False, color=\"pink\")\n",
    "\n",
    "globmean_585_smoothed.plot.line(x=\"time\", ax=ax, color=\"blue\", label=\"ssp585\")\n",
    "globmean_126_smoothed.plot.line(x=\"time\", ax=ax, color=\"red\", label=\"ssp126\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_ssp585 = tas_anom[\"ssp585\"].ds\n",
    "\n",
    "\n",
    "def mask(ds, threshold_land):\n",
    "    ds = mesmer.mask.mask_ocean_fraction(ds, threshold_land)\n",
    "    ds = mesmer.mask.mask_antarctica(ds)\n",
    "    return ds\n",
    "\n",
    "\n",
    "esm_ssp585 = mask(esm_ssp585, THRESHOLD_LAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_ssp585 = esm_ssp585.tas.stack(sample=(\"time\", \"lat\", \"lon\", \"member\"))\n",
    "emu_ssp585 = spatial_emu_585.stack(sample=(\"time\", \"lat\", \"lon\", \"realisation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot_2samples(esm_ssp585, emu_ssp585, line=\"45\")\n",
    "plt.xlabel(\"ESM\")\n",
    "plt.ylabel(\"Emulation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesmer_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
