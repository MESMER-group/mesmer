{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating MESMER on multiple scenarios\n",
    "This tutorial shows how to calibrate the parameters for MESMER on an example dataset of coarse regridded ESM output for multiple climate change scenarios. We calibrate the parameters for MESMER using three scenarios: a historical, a low emission (SSP1-2.6), and a high emission (SSP5-8.5) scenario, with SSP5-8.5 including several ensemble members. Training MESMER consists of four steps:\n",
    "\n",
    "- **global trend**: compute the global temperature trend, including the volcanic influence on historical trends\n",
    "- **global variablity**: estimating the parameters to generate global variability\n",
    "- **local trend**: estimate parameters to translate global mean temperature (including global variability) into locally resolved temperature\n",
    "- **local variability**: estimate parameters needed to generate local variability\n",
    "\n",
    "You can find the basics of the MESMER approach in [Beusch et al. 2020](https://doi.org/10.5194/ESD-11-139-2020) and the multi-sceario approach in [Beusch et al. 2022](https://doi.org/10.5194/gmd-15-2085-2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"tags\": [\"remove-cell\", \"remove-output\"]}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from filefisher import FileFinder\n",
    "\n",
    "import mesmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMSER expects a specific data format. Data from each scenario should be a node (or group) on an `xr.DataTree` (more on this below) e.g.:\n",
    "\n",
    "```\n",
    "<xarray.DataTree>\n",
    "Group: /\n",
    "├── Group: /historical\n",
    "|    ...\n",
    "├── Group: /ssp126\n",
    "|    ...\n",
    "```\n",
    "\n",
    "Each node should be `xr.Dataset` with 4 dimensions: member, time, lat, lon. Below we show one way to load data such that it conforms to the desired format. We load data from the cmip6-ng (\"new generation\") repository. This data has undergone a small reformatting from the original cmip6 archive. For the sake of computational speed we also load data which has been regridded to a coarse resolution. Loading the data can be adapted to the data format you are most used to - as long as the final output has the desired format.\n",
    "\n",
    "---\n",
    "\n",
    "MESMER is Earth System Model specific, aiming to reproduce to some extent the behaviour of one ESM. Here we train on the CMIP6 output of the model IPSL-CM6A-LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"IPSL-CM6A-LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the library [*filefisher*](https://github.com/mpytools/filefisher) to search all files in the cmip6-ng archive for the model and scenarios we want to use. Filefisher can search through paths for given file patterns. It returns all paths matching the pattern such that you can load the files in the next step.\n",
    "\n",
    "Here, we want to find all files that have data for annual near surface temperature (tas) for the used model and the future scenarios ssp126 and ssp585. Next, we search for the historical data that match the members found for the two future scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesmer provides example data under \"data/cmip6-ng\"\n",
    "cmip_data_path = mesmer.example_data.cmip6_ng_path(relative=True)\n",
    "\n",
    "CMIP_FILEFINDER = FileFinder(\n",
    "    path_pattern=cmip_data_path / \"{variable}/{time_res}/{resolution}\",\n",
    "    file_pattern=\"{variable}_{time_res}_{model}_{scenario}_{member}_{resolution}.nc\",\n",
    ")\n",
    "CMIP_FILEFINDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search data for ssp126 and ssp585 - we find one and two ensemble members, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"ssp126\", \"ssp585\"]\n",
    "\n",
    "fc_scens = CMIP_FILEFINDER.find_files(\n",
    "    variable=\"tas\", scenario=scenarios, model=model, resolution=\"g025\", time_res=\"ann\"\n",
    ")\n",
    "\n",
    "fc_scens.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to find the same ensemble members in the historical data, such that we end up with five files we need to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the historical members that are also in the future scenarios, but only once\n",
    "members = fc_scens.df.member.unique()\n",
    "\n",
    "fc_hist = CMIP_FILEFINDER.find_files(\n",
    "    variable=\"tas\",\n",
    "    scenario=\"historical\",\n",
    "    model=model,\n",
    "    resolution=\"g025\",\n",
    "    time_res=\"ann\",\n",
    "    member=members,\n",
    ")\n",
    "\n",
    "fc_all = fc_hist.concat(fc_scens)\n",
    "fc_all.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load all the files we found into a ``DataTree``. ``DataTree`` is a data structure provided by [xarray](https://docs.xarray.dev/en/stable/index.html).\n",
    "\n",
    "Essentially, ``DataTree`` is a container to hold xarray `Dataset` with data variables that are not alignable. This is useful for us since we have historical and future data, which have different time coordinates. Moreover, the scenarios may also have different numbers of members (as e.g., ssp126, which only has one). Thus, we store the data of each sceanrio in a `xarray.Dataset` holding all its ensemble members along a `member` dimension. Then we store all the scenario datasets in one `DataTree`. The `DataTree` allows us to perform computations on each of the datasets in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = xr.DataTree()\n",
    "\n",
    "scenarios_incl_hist = [\"historical\"] + scenarios\n",
    "\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n",
    "\n",
    "# load data for each scenario\n",
    "for scen in scenarios_incl_hist:\n",
    "    files = fc_all.search(scenario=scen)\n",
    "\n",
    "    # load all members for a scenario\n",
    "    members = []\n",
    "    for fN, meta in files.items():\n",
    "        ds = xr.open_dataset(fN, decode_times=time_coder)\n",
    "        # drop unnecessary variables\n",
    "        ds = ds.drop_vars([\"height\", \"time_bnds\", \"file_qf\"], errors=\"ignore\")\n",
    "        # assign member-ID as coordinate\n",
    "        ds = ds.assign_coords({\"member\": meta[\"member\"]})\n",
    "        members.append(ds)\n",
    "\n",
    "    # create a Dataset that holds each member along the member dimension\n",
    "    scen_data = xr.concat(members, dim=\"member\")\n",
    "    # put the scenario dataset into the DataTree\n",
    "    dt[scen] = xr.DataTree(scen_data)\n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in the data format discussed above. You can examine it by clicking on `Groups` above. \n",
    "\n",
    "---\n",
    "\n",
    "We will need some configuration parameters in the follwing:\n",
    "1. ``THRESHOLD_LAND``: Threshold above which land fraction to consider a grid point as a land grid point.\n",
    "2. ``REFERENCE_PERIOD``: We will work not with absolute temperature values but with temperature anomalies w.r.t. a reference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_LAND = 1 / 3\n",
    "REFERENCE_PERIOD = slice(\"1850\", \"1900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies w.r.t. the reference period\n",
    "tas_anom = mesmer.anomaly.calc_anomaly(dt, reference_period=REFERENCE_PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volcanic contributions\n",
    "The volcanic contributions to the global mean temperature trend of the historical period have to be removed in order to estimate the linear regression of global mean temperature to local temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate global mean and smooth the forcing data using a lowess smoother, using 50 time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global mean\n",
    "tas_globmean = mesmer.weighted.global_mean(tas_anom)\n",
    "tas_globmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean over members before smoothing\n",
    "tas_globmean_ensmean = tas_globmean.mean(dim=\"member\")\n",
    "\n",
    "n_steps = 50\n",
    "\n",
    "tas_globmean_smoothed = mesmer.stats.lowess(\n",
    "    tas_globmean_ensmean,\n",
    "    dim=\"time\",\n",
    "    n_steps=n_steps,\n",
    "    use_coords=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "h0, *_ = tas_globmean[\"historical\"].tas.plot.line(ax=ax, x=\"time\", color=\"grey\", lw=1)\n",
    "a2, *_ = tas_globmean_smoothed[\"historical\"].tas.plot.line(ax=ax, x=\"time\", lw=2)\n",
    "\n",
    "ax.legend([h0, a2], [\"Ensemble members\", \"Smooth ensemble mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit the parameter of the volcanic contributions only on the historical smoothed data of all ensemble members available. The future scenarios do not have volcanic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_tas_residuals = tas_globmean[\"historical\"] - tas_globmean_smoothed[\"historical\"]\n",
    "\n",
    "# fit volcanic influence\n",
    "volcanic_params = mesmer.volc.fit_volcanic_influence(hist_tas_residuals.tas)\n",
    "\n",
    "volcanic_params.aod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Superimpose the volcanic influence on the historical time series. Note that due to the approach to handle the historical data as its own scenario, we ecounter discontinuities at the boundary between historical and future period. However, this is not relevant for the fitting of the parameters hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superimpose the volcanic forcing on historical data\n",
    "tas_globmean_smoothed[\"historical\"] = mesmer.volc.superimpose_volcanic_influence(\n",
    "    tas_globmean_smoothed[\"historical\"],\n",
    "    volcanic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plotting\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "# plot unsmoothed global means\n",
    "tas_globmean[\"historical\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"0.5\", add_legend=False\n",
    ")\n",
    "tas_globmean[\"ssp126\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"#6baed6\", add_legend=False\n",
    ")\n",
    "tas_globmean[\"ssp585\"].tas.plot.line(\n",
    "    ax=ax, lw=1, x=\"time\", color=\"#fc9272\", add_legend=False\n",
    ")\n",
    "\n",
    "# plot smoothed global means including volcanic influence for historical\n",
    "tas_globmean_smoothed[\"historical\"].tas.plot.line(\n",
    "    ax=ax, lw=1.5, x=\"time\", color=\"0.1\", label=\"historical\"\n",
    ")\n",
    "tas_globmean_smoothed[\"ssp126\"].tas.plot.line(\n",
    "    ax=ax, lw=1.5, x=\"time\", color=\"#08519c\", label=\"ssp126\"\n",
    ")\n",
    "tas_globmean_smoothed[\"ssp585\"].tas.plot.line(\n",
    "    ax=ax, lw=1.5, x=\"time\", color=\"#de2d26\", label=\"ssp585\"\n",
    ")\n",
    "\n",
    "# histend = tas_globmean[\"historical\"].time.isel(time=-1).item()\n",
    "# ax.axvline(histend, color=\"0.4\")\n",
    "ax.axhline(0, color=\"0.1\", lw=0.5)\n",
    "\n",
    "ax.set_title(\"\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate residuals (w.r.t. smoothed ts) i.e. remove the smoothed global mean, including the volcanic influence from the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_globmean_resids = tas_globmean - tas_globmean_smoothed\n",
    "# rename to tas_resids\n",
    "tas_globmean_resids = mesmer.datatree.map_over_datasets(\n",
    "    lambda ds: ds.rename({\"tas\": \"tas_resids\"}), tas_globmean_resids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "h0, *_ = tas_globmean_resids[\"historical\"].tas_resids.plot.line(\n",
    "    x=\"time\", color=\"0.5\", lw=1, add_legend=False\n",
    ")\n",
    "h1, *_ = tas_globmean_resids[\"ssp126\"].tas_resids.plot.line(\n",
    "    x=\"time\", color=\"#08519c\", lw=1, add_legend=False\n",
    ")\n",
    "h2, *_ = tas_globmean_resids[\"ssp585\"].tas_resids.plot.line(\n",
    "    x=\"time\", color=\"#de2d26\", lw=1, add_legend=False\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Residuals\")\n",
    "plt.axhline(0, lw=1, color=\"0.1\")\n",
    "\n",
    "plt.legend([h0, h1, h2], [\"historical\", \"ssp126\", \"ssp585\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to fit an AR process for estimating global variability, taking in the residual global mean temperature as follows:\n",
    "\n",
    "$T_{t}^{glob, var} = \\alpha_0 + \\sum\\limits_{k=1}^{k=p} \\alpha_k \\cdot T_{t-k}^{glob, var} + \\varepsilon_t,\\ \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the order of the AR process and then fit the parameters. Internally, we fit the parameters for each member and then average first over the parameters of each scenario and then over all scenarios to arrive at a single set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_order = mesmer.stats.select_ar_order_scen_ens(\n",
    "    tas_globmean_resids, dim=\"time\", ens_dim=\"member\", maxlag=12, ic=\"bic\"\n",
    ")\n",
    "\n",
    "global_ar_params = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    tas_globmean_resids, dim=\"time\", ens_dim=\"member\", lags=ar_order\n",
    ")\n",
    "\n",
    "global_ar_params = global_ar_params.drop_vars(\"nobs\")\n",
    "global_ar_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local forced response\n",
    "Now we need to estimate how the global trend translates into a local forced response. This is done using a linear regression of the global trend and the global variability as predictors:\n",
    "\n",
    "$T_{s,t}^{resp} = \\beta_s^{trend} \\cdot T_t^{glob, trend} + \\beta_s^{int} + \\beta_s^{var} \\cdot T_t^{glob, var}$\n",
    "\n",
    "To this end, we stack all values (members, scenarios) into a single dataset, the only important thing is that predictor and predicted values stay together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare the local temperature data:\n",
    "\n",
    "1. Mask out ocean grid points (where the land fraction is larger than `THRESHOLD_LAND`)\n",
    "2. Mask out Antarctica\n",
    "3. Convert the data from a 2D lat-lon grid to a 1D grid by stacking it and removing all gridcells that were previously masked out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before stacking, we extract the original grid. We need to save this together with the parameters to later be able to reconstruct the original grid from the gridpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract original grid\n",
    "grid_orig = tas_anom[\"historical\"].ds[[\"lat\", \"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_stack(dt, threshold_land):\n",
    "    dt = mesmer.mask.mask_ocean_fraction(dt, threshold_land)\n",
    "    dt = mesmer.mask.mask_antarctica(dt)\n",
    "    dt = mesmer.grid.stack_lat_lon(dt)\n",
    "    return dt\n",
    "\n",
    "\n",
    "# mask and stack the data\n",
    "tas_stacked = mask_and_stack(tas_anom, THRESHOLD_LAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_stacked[\"ssp585\"].tas.isel(member=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now converted the 3D field (with dimensions lat, lon, and time) to a 2D field (with dimensions gridcell and time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `DataTree` from all predictors - here the smoothed global mean and it's residuals. We could add more predictors similarly (e.g. the squared temperatures or the ocean heat uptake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = mesmer.datatree.merge([tas_globmean_smoothed, tas_globmean_resids])\n",
    "\n",
    "target = tas_stacked.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression, we want to weight the values of the different scnearios equally, i.e. we do not want scenarios with more members (here ssp585) be overrepresented in the linear regression parameters. Thus, we generate weights that weigh each value by the number of members in their scenario, so $w_{scen, mem, ts} = 1 / n\\_mem_{scen}$. We do currently not take different number of timesteps (historical vs. scenario) into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weights\n",
    "weights = mesmer.weighted.equal_scenario_weights_from_datatree(tas_stacked)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pool the different scenarios, ensemble members and timesteps into one sample dimension for the linear regression. We want one array per predictor and the target such that each sample of the predictor variables aligns with the corresponding sample of the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update after #633\n",
    "predictors_pooled, target_pooled, weights_pooled = (\n",
    "    mesmer.core.datatree.broadcast_and_pool_scen_ens(predictors, target, weights)\n",
    ")\n",
    "\n",
    "target_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the target and predictors in `xr.Dataset`s where the scenario, member and time dimensions are stacked into one sample dimension, i.e. `sample = (scenario, member, time)`. In the linear regression, the predictors for each sample are used for every gridpoint of the target. We can now fit the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_lin_reg = mesmer.stats.LinearRegression()\n",
    "\n",
    "local_lin_reg.fit(\n",
    "    predictors=predictors_pooled,\n",
    "    target=target_pooled.tas,\n",
    "    dim=\"sample\",\n",
    "    weights=weights_pooled.weights,\n",
    ")\n",
    "\n",
    "local_forced_response_params = local_lin_reg.params\n",
    "local_forced_response_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars = (\n",
    "    \"intercept\",\n",
    "    \"tas\",\n",
    "    \"tas_resids\",\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(\n",
    "    3, 1, sharex=True, sharey=True, subplot_kw={\"projection\": ccrs.Robinson()}\n",
    ")\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, data_var in zip(axs, data_vars):\n",
    "\n",
    "    da = local_forced_response_params[data_var]\n",
    "    da = mesmer.grid.unstack_lat_lon_and_align(da, grid_orig)\n",
    "\n",
    "    h = da.plot(\n",
    "        ax=ax,\n",
    "        label=data_var,\n",
    "        robust=True,\n",
    "        center=0,\n",
    "        extend=\"both\",\n",
    "        add_colorbar=False,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "    ax.set_extent((-180, 180, -60, 85), ccrs.PlateCarree())\n",
    "    cbar = plt.colorbar(h, ax=ax, extend=\"both\", pad=0.025)  # , shrink=0.7)\n",
    "    ax.set(title=data_var, xlabel=\"\", ylabel=\"\", xticks=[], yticks=[])\n",
    "    ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local variability\n",
    "Now we need to fit the parameters for the AR(1) process with a spatially correlated noise term used to emulate local variability:\n",
    "\n",
    "$\\eta_{s,t} = \\gamma_{0, s} + \\gamma_{1, 2} \\cdot \\eta_{s, t-1} + \\nu_{s,t}, \\ \\nu_{s,t} \\sim \\mathcal{N}(0, \\Sigma_{\\nu}(r))$\n",
    "\n",
    "The first component that contains the AR parameters $\\gamma_{0, s} + \\gamma_{1, 2} \\cdot \\eta_{s, t-1}$ ensures temporal correlation of the local variability whereas the noise term $\\nu_{s,t}$ ensures spatial consistency. The covariance matrix $\\Sigma_{\\nu}(r)$ is estimated on the whole grid and represents the spatial correlation of temperatures at the different gridpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the AR parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to compute the residuals after the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_lin_reg.residuals(predictors=predictors_pooled, target=target_pooled.tas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_lin_reg.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors\n",
    "\n",
    "local_lin_reg.predict(predictors, exclude=\"tas_resids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = local_lin_reg.residuals(predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_stacked_residuals = local_lin_reg.residuals(\n",
    "    predictors=predictors_pooled, target=target_pooled.tas\n",
    ").T\n",
    "\n",
    "tas_stacked_residuals.T.plot()\n",
    "\n",
    "# unstack the residuals\n",
    "tas_un_stacked_residuals = tas_stacked_residuals.set_index(\n",
    "    sample=(\"time\", \"member\", \"scenario\")\n",
    ").unstack(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_un_stacked_residuals = tas_un_stacked_residuals.rename(\"residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_un_stacked_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put each scenario into a DataTree node again, get rid of superfluous (nan) time steps and/or members\n",
    "# that result from the unstacking\n",
    "dt_resids = xr.DataTree()\n",
    "for scenario in tas_un_stacked_residuals.scenario.values:\n",
    "    dt_resids[scenario] = xr.DataTree(\n",
    "        tas_un_stacked_residuals.to_dataset()\n",
    "        .sel(scenario=scenario)\n",
    "        .dropna(\"member\", how=\"all\")\n",
    "        .dropna(\"time\")\n",
    "        .drop_vars(\"scenario\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_resids\n",
    "\n",
    "# xr.testing.assert_equal(\n",
    "#     dt_resids,\n",
    "#     mesmer.datatree.map_over_datasets(lambda ds: ds.rename(tas=\"residuals\"), resids),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the AR(1) process\n",
    "local_ar = mesmer.stats.fit_auto_regression_scen_ens(\n",
    "    dt_resids,\n",
    "    ens_dim=\"member\",\n",
    "    dim=\"time\",\n",
    "    lags=1,\n",
    ")\n",
    "\n",
    "local_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate covariance matrix\n",
    "For the covariance matrix of the white noise we first estimate the empirical covariance matrix of the gridcell's values and then localize it using the Gaspari-Cohn function. This function goes to 0 for for distances bigger than the so called localisation radius. This is also called regularization. It ensures that grid points that are further away from each other do not correlate. Such spurious correlations can arise from rank deficient covariance matrices. In our case because we estimate the covariance on data that has more gridcells than timesteps.\n",
    "\n",
    "The localisation radius is a parameter that needs to be calibrated and we find the best localisation radius by cross-validation of several radii using the negative loglikelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the distance matrix - the distance between the gridpoints in km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt_resids[\"historical\"].ds\n",
    "geodist = mesmer.geospatial.geodist_exact(dt.lon, dt.lat)\n",
    "\n",
    "# plot\n",
    "f, ax = plt.subplots()\n",
    "geodist.plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. prepare the localizer(s) to regularize the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_gc_localizer = mesmer.stats.gaspari_cohn_correlation_matrices(\n",
    "    geodist, range(5_000, 15_001, 500)\n",
    ")\n",
    "\n",
    "# plot one\n",
    "f, ax = plt.subplots()\n",
    "phi_gc_localizer[5000].plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing weights from local trend regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. find the best localization radius and localize the empirical covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = \"sample\"\n",
    "k_folds = 15\n",
    "\n",
    "localized_ecov = mesmer.stats.find_localized_empirical_covariance(\n",
    "    tas_stacked_residuals, weights_pooled.weights, phi_gc_localizer, dim, k_folds\n",
    ")\n",
    "\n",
    "localized_ecov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, sharey=True, constrained_layout=True)\n",
    "\n",
    "opt = dict(vmin=0, vmax=1.5, cmap=\"Blues\", add_colorbar=False)\n",
    "\n",
    "ax = axs[0]\n",
    "localized_ecov.covariance.plot(ax=ax, **opt)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Empirical covariance\")\n",
    "\n",
    "ax = axs[1]\n",
    "localized_ecov.localized_covariance.plot(ax=ax, **opt)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Localized empirical covariance\")\n",
    "ax.set_ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Adjust the regularized covariance matrix\n",
    "\n",
    "Lastly we need to adjust the localized covariance matrix using the AR(1) parameters since the variance of the time series we observe is bigger than the variance of the driving white noise process. Read more about this here: \"Statistical Analysis in Climate Research\" by Stroch and Zwiers (1999, reprinted 2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localized_covariance_adjusted = mesmer.stats.adjust_covariance_ar1(\n",
    "    localized_ecov.localized_covariance, local_ar.coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have calibrated all needed parameters and can save them. We can use filefisher to nicely create file names and save the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path relative to this notebook & create folder\n",
    "param_path = pathlib.Path(\"./output/calibrated_parameters/\")\n",
    "param_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_FILEFINDER = FileFinder(\n",
    "    path_pattern=param_path,\n",
    "    file_pattern=\"params_{module}_{esm}_{scen}.nc\",\n",
    ")\n",
    "\n",
    "scen_str = \"-\".join(scenarios)\n",
    "\n",
    "params = {\n",
    "    \"volcanic\": volcanic_params,\n",
    "    \"global-variability\": global_ar_params,\n",
    "    \"local-trends\": local_lin_reg,\n",
    "    \"local-variability\": local_ar,\n",
    "    \"covariance\": localized_ecov,\n",
    "}\n",
    "\n",
    "\n",
    "save_files = False  # we don't save them here in the example\n",
    "if save_files:\n",
    "\n",
    "    for module, param in params.items():\n",
    "\n",
    "        filename = PARAM_FILEFINDER.create_full_name(\n",
    "            module=module,\n",
    "            esm=model,\n",
    "            scen=scen_str,\n",
    "        )\n",
    "\n",
    "        pathlib.Path(filename).parent.mkdir(exist_ok=True)\n",
    "        param.to_netcdf(filename)\n",
    "\n",
    "# TODO: save the original grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff0800\">D</span><span style=\"color:#ffb700\">o</span><span style=\"color:#6df921\">n</span><span style=\"color:#00ffff\">e</span><span style=\"color:#fb00ff\">!</span>\n",
    "When you want to use the calibrated parameters for emulation, see the Tutorials for emulating one or multiple scnearios in the Tutorial section next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesmer_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
